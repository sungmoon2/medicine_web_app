"""
ë„¤ì´ë²„ API í˜¸ì¶œ í´ë¼ì´ì–¸íŠ¸
"""
import os
import json
import time
import urllib.request
import urllib.parse
import urllib.error
import requests
import random

from datetime import datetime
from config.settings import (
    NAVER_CLIENT_ID, NAVER_CLIENT_SECRET, 
    REQUEST_DELAY, MAX_RETRIES,
    DAILY_API_LIMIT, SEARCH_DEFAULTS
)
from utils.helpers import retry
from utils.logger import get_logger

# ë¡œê±° ì„¤ì •
logger = get_logger(__name__)

class NaverAPIClient:
    def __init__(self, db_manager=None):
        self.client_id = NAVER_CLIENT_ID
        self.client_secret = NAVER_CLIENT_SECRET
        self.db_manager = db_manager
        self.today_api_calls = 0
        self.session = requests.Session()
        
        # ğŸ”¹ ì„¸ì…˜ì„ í†µí•œ ë„¤ì´ë²„ ì²« í˜ì´ì§€ ì ‘ê·¼ â†’ ì¿ í‚¤ ìœ ì§€
        self.session.get("https://www.naver.com", timeout=5)

        # ğŸ”¹ User-Agent ëœë¤í™”
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/99.0.4844.84 Safari/537.36",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36",
            "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/97.0.4692.99 Safari/537.36"
        ]

        self.session.headers.update({
            'User-Agent': random.choice(self.user_agents),  # ğŸ”¹ ëœë¤ User-Agent
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'https://www.naver.com',  # ğŸ”¹ ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ë¥¼ Refererë¡œ ì¶”ê°€
            'Origin': 'https://www.naver.com'  # ğŸ”¹ Origin ì¶”ê°€
        })
        
        # í˜„ì¬ URL ì €ì¥ ë³€ìˆ˜ (ë¦¬ë‹¤ì´ë ‰íŠ¸ ì¶”ì ìš©)
        self.current_url = None
        
        # ì˜¤ëŠ˜ API í˜¸ì¶œ íšŸìˆ˜ ë¡œë“œ
        self._load_today_api_calls()

    def _random_delay(self, min_delay=1, max_delay=3):
        """ìš”ì²­ ê°„ ëœë¤ ì§€ì—° ì¶”ê°€ (1~3ì´ˆ ì‚¬ì´)"""
        delay = random.uniform(min_delay, max_delay)
        logger.info(f"ëœë¤ ëŒ€ê¸° ì‹œê°„: {delay:.2f}ì´ˆ")
        time.sleep(delay)

    def search_medicine(self, keyword, display=None, start=1):
        # ìš”ì²­ ê°„ ëœë¤ ì§€ì—° ì¶”ê°€
        self._random_delay()

        try:
            response = self.session.get(url, headers=headers, timeout=10)
            response.raise_for_status()  # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
            return response.json()
        except requests.RequestException as e:
            logger.error(f"ë„¤ì´ë²„ API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None
        
    def get_html_content(self, url, follow_redirects=True, max_retries=3):
        # ëœë¤ ëŒ€ê¸° ì‹œê°„ ì ìš©
        self._random_delay()

        try:
            response = self.session.get(url, headers=headers, timeout=15)
            response.raise_for_status()  # HTTP ì—ëŸ¬ ë°œìƒ ì‹œ ì˜ˆì™¸ ë°œìƒ
            return response.text
        except requests.RequestException as e:
            logger.error(f"HTML í˜ì´ì§€ ê°€ì ¸ì˜¤ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return None


    def _load_today_api_calls(self):
        """ì˜¤ëŠ˜ì˜ API í˜¸ì¶œ íšŸìˆ˜ ë¡œë“œ"""
        if self.db_manager:
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ API í˜¸ì¶œ íšŸìˆ˜ ê°€ì ¸ì˜¤ê¸°
            today = datetime.now().strftime('%Y-%m-%d')
            count = self.db_manager.get_api_call_count(today)
            if count is not None:
                self.today_api_calls = count
        else:
            self.today_api_calls = 0
            
    def _update_api_call_count(self, count=1):
        """
        API í˜¸ì¶œ íšŸìˆ˜ ì—…ë°ì´íŠ¸
        
        Args:
            count: ì¦ê°€ì‹œí‚¬ í˜¸ì¶œ íšŸìˆ˜
        
        Returns:
            int: ì—…ë°ì´íŠ¸ í›„ ì˜¤ëŠ˜ì˜ ì´ API í˜¸ì¶œ íšŸìˆ˜
        """
        self.today_api_calls += count
        
        if self.db_manager:
            # ë°ì´í„°ë² ì´ìŠ¤ì— API í˜¸ì¶œ íšŸìˆ˜ ì—…ë°ì´íŠ¸
            today = datetime.now().strftime('%Y-%m-%d')
            self.db_manager.update_api_call_count(today, self.today_api_calls)
        
        return self.today_api_calls
    
    def check_api_limit(self):
        """
        API í˜¸ì¶œ í•œë„ì— ë„ë‹¬í–ˆëŠ”ì§€ í™•ì¸
        
        Returns:
            bool: API í˜¸ì¶œ í•œë„ì— ë„ë‹¬í–ˆìœ¼ë©´ True, ì•„ë‹ˆë©´ False
        """
        return self.today_api_calls >= DAILY_API_LIMIT
    
    @retry(max_tries=MAX_RETRIES, delay_seconds=REQUEST_DELAY, backoff_factor=2, 
       exceptions=(requests.RequestException, urllib.error.URLError))
    def search_medicine(self, keyword, display=None, start=1):
        """
        ë„¤ì´ë²„ APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì•½í’ˆ ê²€ìƒ‰
        
        Args:
            keyword: ê²€ìƒ‰ í‚¤ì›Œë“œ 
            display: í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê²°ê³¼ ìˆ˜ (ìµœëŒ€ 100)
            start: ê²€ìƒ‰ ì‹œì‘ ìœ„ì¹˜
            
        Returns: 
            dict: API ì‘ë‹µ ë°ì´í„° ë˜ëŠ” None (ì—ëŸ¬ ë°œìƒ ì‹œ)
        """
        if display is None:
            display = SEARCH_DEFAULTS['display']
            
        # ê¸°ë³¸ê°’ í™•ì¸
        display = min(display, 100)  # ìµœëŒ€ 100ê°œê¹Œì§€ë§Œ ê°€ëŠ¥
        
        # API í•œë„ ì²´í¬
        if self.check_api_limit():
            logger.warning(f"ì¼ì¼ API í˜¸ì¶œ í•œë„({DAILY_API_LIMIT}íšŒ)ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤.")
            return None
        
        # ê²€ìƒ‰ì–´ êµ¬ì„±
        search_query = f"{keyword} ì˜ì•½í’ˆ"
        encoded_query = urllib.parse.quote(search_query)
        
        # 'encyclop.json' ëŒ€ì‹  'encyc.json' ì‚¬ìš©
        url = f"https://openapi.naver.com/v1/search/encyc.json?query={encoded_query}&display={display}&start={start}"
        
        # ìš”ì²­ í—¤ë” ì„¤ì •
        headers = {
            "X-Naver-Client-Id": self.client_id,
            "X-Naver-Client-Secret": self.client_secret
        }
        
        logger.info(f"API ìš”ì²­: í‚¤ì›Œë“œ='{keyword}', display={display}, start={start}")
        
        try:
            # API ìš”ì²­
            response = self.session.get(url, headers=headers, timeout=10)
            response.raise_for_status()
            
            # JSON íŒŒì‹±
            result = response.json()
            
            # ê²°ê³¼ ì •ë³´ ë¡œê¹…
            if 'total' in result:
                logger.info(f"API ê²€ìƒ‰ ê²°ê³¼: ì´ {result['total']}ê°œ í•­ëª© ì¤‘ {len(result.get('items', []))}ê°œ ë°˜í™˜ë¨")
            else:
                logger.warning(f"API ì‘ë‹µì— 'total' í•„ë“œê°€ ì—†ìŒ")
            
            # API í˜¸ì¶œ ì¹´ìš´í„° ì—…ë°ì´íŠ¸
            self._update_api_call_count()
            
            # ìš”ì²­ ê°„ ë”œë ˆì´ ì¶”ê°€
            time.sleep(REQUEST_DELAY)
            
            return result
            
        except json.JSONDecodeError as e:
            logger.error(f"API ì‘ë‹µì„ JSONìœ¼ë¡œ íŒŒì‹±í•  ìˆ˜ ì—†ìŒ: {e}")
            raise
            
        except requests.RequestException as e:
            logger.error(f"API ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
            # ì˜¤ë¥˜ ìƒì„¸ ì •ë³´ ë¡œê¹…
            if hasattr(e, 'response') and e.response is not None:
                logger.error(f"ì˜¤ë¥˜ ìƒíƒœ ì½”ë“œ: {e.response.status_code}")
                logger.error(f"ì˜¤ë¥˜ ì‘ë‹µ ë‚´ìš©: {e.response.text}")
            
            raise
    
    @retry(max_tries=3, delay_seconds=1, exceptions=(requests.RequestException,))
    def get_html_content(self, url, follow_redirects=True, max_retries=3):
        """
        ì£¼ì–´ì§„ URLì—ì„œ HTML ë‚´ìš© ê°€ì ¸ì˜¤ê¸° (ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²˜ë¦¬ ê°œì„ )
        
        Args:
            url: ê°€ì ¸ì˜¬ ì›¹í˜ì´ì§€ URL
            follow_redirects: ë¦¬ë‹¤ì´ë ‰íŠ¸ ë”°ë¼ê°€ê¸° ì—¬ë¶€
            max_retries: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
            
        Returns:
            str: ì›¹í˜ì´ì§€ HTML ë‚´ìš© ë˜ëŠ” None (ì—ëŸ¬ ë°œìƒ ì‹œ)
        """
        # API í•œë„ ì²´í¬ (HTML ìš”ì²­ë„ ì¹´ìš´íŠ¸)
        if self.check_api_limit():
            logger.warning("ì¼ì¼ ìš”ì²­ í•œë„ì— ë„ë‹¬í–ˆìŠµë‹ˆë‹¤")
            return None
        
        # URL ì •ë³´ ì¶”ì¶œ
        parsed_url = urllib.parse.urlparse(url)
        domain = f"{parsed_url.scheme}://{parsed_url.netloc}"
        
        session = requests.Session()
        
        # ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜
        for attempt in range(max_retries):
            try:
                # ìš”ì²­ í—¤ë” ì„¤ì • (ë¸Œë¼ìš°ì €ì²˜ëŸ¼ ë³´ì´ë„ë¡)
                headers = {
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36",
                    "Referer": "https://www.naver.com/",  # ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ì—ì„œ ì ‘ê·¼í•œ ê²ƒì²˜ëŸ¼ ë³´ì´ë„ë¡ ì„¤ì •
                    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                    'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
                    'Referer': domain,
                    'Connection': 'keep-alive',
                    'Upgrade-Insecure-Requests': '1'
                }
                
                # ì§ì ‘ ìš”ì²­ (ë¦¬ë‹¤ì´ë ‰íŠ¸ í—ˆìš©)
                response = self.session.get(
                    url, 
                    headers=headers,
                    allow_redirects=follow_redirects,
                    timeout=15
                )
                
                # ì‹¤ì œ URL ì €ì¥ (ë¦¬ë‹¤ì´ë ‰íŠ¸ í›„)
                self.current_url = response.url
                
                # ìƒíƒœ ì½”ë“œ í™•ì¸
                if response.status_code == 200:
                    # ì¸ì½”ë”© ì²˜ë¦¬
                    response.encoding = response.apparent_encoding
                    html_content = response.text
                    
                    # ê°„ë‹¨í•œ HTML ìœ íš¨ì„± ê²€ì‚¬
                    if '<html' in html_content.lower() and len(html_content) > 1000:
                        # ë””ë²„ê·¸ ì •ë³´
                        logger.debug(f"HTML ê°€ì ¸ì˜¤ê¸° ì„±ê³µ: URL {url} â†’ {response.url if url != response.url else url}")
                        
                        # API í˜¸ì¶œ ì¹´ìš´í„° ì—…ë°ì´íŠ¸
                        self._update_api_call_count()
                        
                        return html_content
                    else:
                        logger.warning(f"HTML ë‚´ìš©ì´ ìœ íš¨í•˜ì§€ ì•ŠìŒ: URL {url}, ê¸¸ì´ {len(html_content)}")
                        # ë§‰íŒ í˜ì´ì§€ ë˜ëŠ” ë¹„ì •ìƒ ì‘ë‹µ ì²˜ë¦¬
                        if len(html_content) < 1000:
                            logger.debug(f"ì§§ì€ ì‘ë‹µ ë‚´ìš©: {html_content[:200]}")
                
                # ë¦¬ë‹¤ì´ë ‰íŠ¸ ì²˜ë¦¬
                elif response.status_code in (301, 302, 303, 307, 308):
                    if not follow_redirects:
                        logger.info(f"ë¦¬ë‹¤ì´ë ‰íŠ¸ ê°ì§€: {url} â†’ {response.headers.get('Location')}")
                        return None
                    else:
                        logger.warning(f"ë¦¬ë‹¤ì´ë ‰íŠ¸ í›„ì—ë„ ì„±ê³µí•˜ì§€ ëª»í•¨: {url} â†’ {response.url}")
                
                # 404 ì˜¤ë¥˜
                elif response.status_code == 404:
                    logger.warning(f"í˜ì´ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ (404): {url}")
                    return None
                
                # ë‹¤ë¥¸ ì˜¤ë¥˜
                else:
                    logger.warning(f"HTTP ì˜¤ë¥˜: ìƒíƒœ ì½”ë“œ {response.status_code}, URL {url}, ì‹œë„ {attempt+1}/{max_retries}")
                
                # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì¬ì‹œë„
                if attempt < max_retries - 1:
                    wait_time = REQUEST_DELAY * (attempt + 1)  # ì ì§„ì  ëŒ€ê¸° ì‹œê°„
                    logger.info(f"{wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                else:
                    logger.error(f"HTML ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: URL {url}, ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
                    return None
                    
            except requests.RequestException as e:
                logger.error(f"ìš”ì²­ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {url}, {e}, ì‹œë„ {attempt+1}/{max_retries}")
                
                # ë§ˆì§€ë§‰ ì‹œë„ê°€ ì•„ë‹ˆë©´ ì¬ì‹œë„
                if attempt < max_retries - 1:
                    wait_time = REQUEST_DELAY * (attempt + 1)  # ì ì§„ì  ëŒ€ê¸° ì‹œê°„
                    logger.info(f"{wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                else:
                    return None
        
        return None

    def _is_valid_url(self, url):
        """
        URL ìœ íš¨ì„± ê²€ì‚¬
        
        Args:
            url: ê²€ì‚¬í•  URL
            
        Returns:
            bool: ìœ íš¨í•œ URLì´ë©´ True
        """
        try:
            result = urllib.parse.urlparse(url)
            return all([result.scheme, result.netloc])
        except Exception:
            return False
        
    def verify_url_is_medicine(self, url):
        """
        URLì´ ì˜ì•½í’ˆ í˜ì´ì§€ì¸ì§€ ê²€ì¦
        
        Args:
            url: ê²€ì¦í•  URL
            
        Returns:
            bool: ì˜ì•½í’ˆ í˜ì´ì§€ë©´ True
        """
        try:
            html_content = self.get_html_content(url)
            if not html_content:
                return False
            
            # ê°„ë‹¨í•œ íŒ¨í„´ ê²€ì‚¬
            patterns = [
                'cid=51000',  # URLì— ì˜ì•½í’ˆ ì¹´í…Œê³ ë¦¬ ID í¬í•¨
                'ì˜ì•½í’ˆì‚¬ì „',    # ì˜ì•½í’ˆì‚¬ì „ í‚¤ì›Œë“œ í¬í•¨
                'medicine'    # medicine í‚¤ì›Œë“œ í¬í•¨
            ]
            
            return any(pattern in html_content for pattern in patterns)
            
        except Exception as e:
            logger.error(f"URL ê²€ì¦ ì¤‘ ì˜¤ë¥˜: {url}, {e}")
            return False